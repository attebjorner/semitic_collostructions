{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import scipy.stats as stats\n",
    "import math"
   ]
  },
  {
   "source": [
    "Чтение conllu-файла с корпусом иврита"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hebrew.conllu', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text = text.split('п')\n",
    "for i in range(0, len(text)):\n",
    "    text[i] = text[i].split('\\n')\n",
    "    text[i][1] = text[i][1][9:]\n",
    "    del text[i][0]\n",
    "    for j in range(1, len(text[i])):\n",
    "        text[i][j] = text[i][j].split('\\t')\n",
    "        text[i][j] = text[i][j][:6]\n",
    "        del text[i][j][4]"
   ]
  },
  {
   "source": [
    "Чтение conllu-файла с корпусом арабского языка"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('arabic.conllu', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text = text.split('п')\n",
    "for i in range(0, len(text)):\n",
    "    text[i] = text[i].split('\\n')\n",
    "    text[i][1] = text[i][1][9:]\n",
    "    del text[i][0]\n",
    "    for j in range(1, len(text[i])):\n",
    "        text[i][j] = text[i][j].split('\\t')\n",
    "        for k in range(0,3):\n",
    "            del text[i][j][6]"
   ]
  },
  {
   "source": [
    "Очистка от пунктуации"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = text\n",
    "for sen in range(0, len(text2)):\n",
    "    del_ids = []\n",
    "    for word in range(1, len(text2[sen])):\n",
    "        if ('-' in text2[sen][word][0]) or (text2[sen][word][3] == 'PUNCT'):\n",
    "            del_ids.append(word)\n",
    "    del_ids.reverse()\n",
    "    for i in del_ids:\n",
    "        del text[sen][i]"
   ]
  },
  {
   "source": [
    "Поиск смихутов  в иврите"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smix = []\n",
    "for sen in range(0, len(text2)):\n",
    "    for word in range(1, len(text2[sen]) - 1):\n",
    "        if ('Definite=Cons' in text2[sen][word][4]) \\\n",
    "                and (text2[sen][word + 1][3] == 'NOUN') \\\n",
    "                and (text2[sen][word][3] == 'NOUN'):\n",
    "            smix.append([sen, word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smix_def = []\n",
    "for sen in range(0, len(text2)):\n",
    "    for word in range(1, len(text2[sen]) - 2):\n",
    "        if ('Definite=Cons' in text2[sen][word][4]) \\\n",
    "                and (text2[sen][word + 1][2]  == 'ה') \\\n",
    "                and (text2[sen][word][3] == 'NOUN') \\\n",
    "                and (text[sen][word+2][3] == 'NOUN'):\n",
    "            smix_def.append([sen, word])"
   ]
  },
  {
   "source": [
    "Поиск местоименных суффиксов в иврите"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suf = []\n",
    "for sen in range(0, len(text2)):\n",
    "    for word in range(1, len(text2[sen]) - 2):\n",
    "        if (text2[sen][word][3] == 'NOUN') \\\n",
    "                and (text2[sen][word + 1][1] == '_של_') \\\n",
    "                and (text2[sen][word + 2][3] == 'PRON'):\n",
    "            suf.append([sen, word])"
   ]
  },
  {
   "source": [
    "Поиск аналитических конструкций в иврите"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_possed_nouns(sen, position):\n",
    "    for word in range(position-1, 0, -1):\n",
    "        if sen[word][3] == 'NOUN':\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shel = []\n",
    "for sen in range(0, len(text2)):\n",
    "    for word in range(1, len(text2[sen]) - 1):\n",
    "        if ((text2[sen][word][1] == 'של_') \\\n",
    "                and (text2[sen][word + 1][3] == 'PRON')) \\\n",
    "                or ((text2[sen][word][1] == 'של') \\\n",
    "                and (text2[sen][word + 1][3] == 'NOUN')):\n",
    "            possed_noun = find_possed_nouns(text2[sen], word)\n",
    "            shel.append([sen, word, possed_noun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shel_def = []\n",
    "for sen in range(0, len(text2)):\n",
    "    for word in range(1, len(text2[sen]) - 2):\n",
    "        if (text2[sen][word][1] == 'של') \\\n",
    "                and (text2[sen][word + 2][3] == 'NOUN') \\\n",
    "                and (text2[sen][word + 1][2] == 'ה'):\n",
    "            possed_noun = find_possed_nouns(text2[sen], word)\n",
    "            shel_def.append([sen, word, possed_noun])"
   ]
  },
  {
   "source": [
    "Поиск идаф в арабском"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idafa = []\n",
    "fst = 'N.{8}R'\n",
    "snd = 'N.{7}2.'\n",
    "for sen in range(0, len(text2)):\n",
    "    for word in range(1, len(text2[sen]) - 1):\n",
    "        if re.match(fst, text2[sen][word][4]) \\\n",
    "                and re.match(snd, text2[sen][word+1][4]):\n",
    "            idafa.append([sen, word])"
   ]
  },
  {
   "source": [
    "Поиск конструкций с местоименными суффиксами в арабском"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suf = []\n",
    "noun = 'N.{8}R'\n",
    "pron = 'SP.{8}'\n",
    "for sen in range(0, len(text2)):\n",
    "    for word in range(1, len(text2[sen]) - 1):\n",
    "        if re.match(noun, text2[sen][word][4]) \\\n",
    "                and re.match(pron, text2[sen][word+1][4]):\n",
    "            suf.append([sen, word])"
   ]
  },
  {
   "source": [
    "Запись полученных данных в csv-файлы"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_ids = []\n",
    "sen = []\n",
    "words = []\n",
    "word_constr = []\n",
    "lemma1 = []\n",
    "posessor = []\n",
    "lemma2 = []\n",
    "for i in smix:\n",
    "    sen_ids.append(i[0])\n",
    "    sen.append(text2[i[0]][0])\n",
    "    words.append(text2[i[0]][i[1]][1] + ' ' + text2[i[0]][i[1] + 1][1])\n",
    "    word_constr.append(text2[i[0]][i[1]][1])\n",
    "    lemma1.append(text2[i[0]][i[1]][2])\n",
    "    posessor.append(text2[i[0]][i[1] + 1][1])\n",
    "    lemma2.append(text2[i[0]][i[1] + 1][2])\n",
    "\n",
    "datas = {\n",
    "    'sen_id': sen_ids,\n",
    "    'sen': sen,\n",
    "    'constr': words,\n",
    "    'word_constr': word_constr,\n",
    "    'lemma_cons': lemma1,\n",
    "    'posessor': posessor,\n",
    "    'lemma_pos-r': lemma2\n",
    "}\n",
    "\n",
    "smix_df = pd.DataFrame(datas)\n",
    "\n",
    "for key in datas.keys():\n",
    "    datas[key] = []\n",
    "for i in smix_def:\n",
    "    datas['sen_ids'].append(i[0])\n",
    "    datas['sen'].append(text2[i[0]][0])\n",
    "    datas['constr'].append(text2[i[0]][i[1]][1] + ' ה' + text2[i[0]][i[1] + 2][1])\n",
    "    datas['word_constr'].append(text2[i[0]][i[1]][1])\n",
    "    datas['lemma1'].append(text2[i[0]][i[1]][2])\n",
    "    datas['posessor'].append('ה' + text2[i[0]][i[1] + 2][1])\n",
    "    datas['lemma2'].append(text2[i[0]][i[1] + 2][2])\n",
    "\n",
    "smix_def_df = pd.DataFrame(datas)\n",
    "\n",
    "smix_df.to_csv('smix.tsv', sep='\\t')\n",
    "smix_def_df.to_csv('smix_def.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_ids = []\n",
    "sen = []\n",
    "words = []\n",
    "pos_ed = []\n",
    "lemma_p_ed = []\n",
    "pos_or = []\n",
    "for i in suf:\n",
    "    sen_ids.append(i[0])\n",
    "    sen.append(text2[i[0]][0])\n",
    "    words.append(text2[i[0]][i[1]][1] + ' ' + text2[i[0]][i[1] + 1][1] + ' ' + text2[i[0]][i[1] + 2][1])\n",
    "    pos_ed.append(text2[i[0]][i[1]][1])\n",
    "    lemma_p_ed.append(text2[i[0]][i[1]][2])\n",
    "    pos_or.append(text2[i[0]][i[1] + 2][1])\n",
    "\n",
    "datas = {\n",
    "    'sen_id': sen_ids,\n",
    "    'sen': sen,\n",
    "    'constr': words,\n",
    "    'pos_ed': pos_ed,\n",
    "    'lemma_p_ed': lemma_p_ed,\n",
    "    'pos_or': pos_or,\n",
    "}\n",
    "\n",
    "suf_df = pd.DataFrame(datas)\n",
    "\n",
    "suf_df.to_csv('suf_heb.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_ids = []\n",
    "sen = []\n",
    "words = []\n",
    "left = []\n",
    "lemma1 = []\n",
    "right = []\n",
    "lemma2 = []\n",
    "\n",
    "for i in shel:\n",
    "    sen_ids.append(i[0])\n",
    "    sen.append(text2[i[0]][0])\n",
    "    words.append(text2[i[0]][i[1]][1] + ' ' + text2[i[0]][i[1] + 1][1])\n",
    "    if i[1] == 1:\n",
    "        left.append('empty')\n",
    "        lemma1.append('empty')\n",
    "    else:\n",
    "        try:\n",
    "            left.append(text2[i[0]][i[2]][1])\n",
    "            lemma1.append(text2[i[0]][i[2]][2])\n",
    "        except:\n",
    "            left.append('empty')\n",
    "            lemma1.append('empty')\n",
    "    right.append(text2[i[0]][i[1] + 1][1])\n",
    "    lemma2.append(text2[i[0]][i[1] + 1][2])\n",
    "\n",
    "datas = {\n",
    "    'sen_id': sen_ids,\n",
    "    'sen': sen,\n",
    "    'constr': words,\n",
    "    'left': left,\n",
    "    'lemma_left': lemma1,\n",
    "    'posessor': right,\n",
    "    'lemma_pos_r': lemma2\n",
    "}\n",
    "\n",
    "shel_df = pd.DataFrame(datas)\n",
    "\n",
    "for key in datas.keys():\n",
    "    datas[key] = []\n",
    "for i in shel_def:\n",
    "    datas['sen_ids'].append(i[0])\n",
    "    datas['sen'].append(text2[i[0]][0])\n",
    "    datas['constr'].append(text2[i[0]][i[1]][1] + ' ה' + text2[i[0]][i[1] + 2][1])\n",
    "    if i[1] == 1:\n",
    "        datas['left'].append('empty')\n",
    "        datas['lemma_left'].append('empty')\n",
    "    else:\n",
    "        try:\n",
    "            datas['left'].append(text2[i[0]][i[2]][1])\n",
    "            datas['lemma_left'].append(text2[i[0]][i[2]][2])\n",
    "        except:\n",
    "            datas['left'].append('empty')\n",
    "            datas['lemma_left'].append('empty')\n",
    "    datas['posessor'].append('ה' + text2[i[0]][i[1] + 2][1])\n",
    "    datas['lemma_pos_r'].append(text2[i[0]][i[1] + 2][2])\n",
    "\n",
    "shel_def_df = pd.DataFrame(datas)\n",
    "\n",
    "shel_df.to_csv('shel.tsv', sep='\\t')\n",
    "shel_def_df.to_csv('shel_def.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_ids = []\n",
    "sen = []\n",
    "words = []\n",
    "word_constr = []\n",
    "lemma1 = []\n",
    "posessor = []\n",
    "lemma2 = []\n",
    "for i in idafa:\n",
    "    sen_ids.append(i[0])\n",
    "    sen.append(text2[i[0]][0])\n",
    "    words.append(text2[i[0]][i[1]][1] + ' ' + text2[i[0]][i[1] + 1][1])\n",
    "    word_constr.append(text2[i[0]][i[1]][1])\n",
    "    lemma1.append(text2[i[0]][i[1]][2])\n",
    "    posessor.append(text2[i[0]][i[1] + 1][1])\n",
    "    lemma2.append(text2[i[0]][i[1] + 1][2])\n",
    "\n",
    "datas = {\n",
    "    'sen_id': sen_ids,\n",
    "    'sen': sen,\n",
    "    'constr': words,\n",
    "    'word_constr': word_constr,\n",
    "    'lemma_cons': lemma1,\n",
    "    'posessor': posessor,\n",
    "    'lemma_pos-r': lemma2\n",
    "}\n",
    "\n",
    "idafa_df = pd.DataFrame(datas)\n",
    "\n",
    "idafa_df.to_csv('idafa.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_ids = []\n",
    "sen = []\n",
    "words = []\n",
    "pos_ed = []\n",
    "lemma_p_ed = []\n",
    "pos_or = []\n",
    "xpos = []\n",
    "for i in suf:\n",
    "    sen_ids.append(i[0])\n",
    "    sen.append(text2[i[0]][0])\n",
    "    words.append(text2[i[0]][i[1]][1] + ' ' + text2[i[0]][i[1] + 1][1])\n",
    "    pos_ed.append(text2[i[0]][i[1]][1])\n",
    "    lemma_p_ed.append(text2[i[0]][i[1]][2])\n",
    "    pos_or.append(text2[i[0]][i[1] + 1][1])\n",
    "    xpos.append(text2[i[0]][i[1] + 1][4][5:9])\n",
    "\n",
    "datas = {\n",
    "    'sen_id': sen_ids,\n",
    "    'sen': sen,\n",
    "    'constr': words,\n",
    "    'pos_ed': pos_ed,\n",
    "    'lemma_p_ed': lemma_p_ed,\n",
    "    'pos_or': pos_or,\n",
    "    'pos_or_xpos': xpos\n",
    "}\n",
    "\n",
    "suf_df = pd.DataFrame(datas)\n",
    "\n",
    "suf_df.to_csv('suf_ara.tsv', sep='\\t')"
   ]
  },
  {
   "source": [
    "Тест Фишера для колексемного анализа\n",
    "\n",
    "Входные данные имеют формат:\n",
    "\n",
    "|лемма|частота|\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frequency_list(filename) -> list:\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        frequency_list = f.read().split('\\n')\n",
    "    for i in range(0, len(suf_lem_c)):\n",
    "        frequency_list[i] = frequency_list[i].split('\\\\')\n",
    "        return frequency_list\n",
    "\n",
    "def count_S(lemma, text) -> int:\n",
    "    c = 0\n",
    "    for i in text:\n",
    "        for j in range(1, len(i)):\n",
    "            if (i[j][2] == lemma) and (i[j][3] == 'NOUN'):\n",
    "                c += 1\n",
    "    return c\n",
    "\n",
    "# k, t -- кол-во всех вхождений конструкции в корпус\n",
    "# и кол-во абсолютно всех конструкций в корпусе\n",
    "# соотвественно (величины известны заранее)\n",
    "def count_fisher(frequency_list, k: int, t: int) -> list:\n",
    "    data = []\n",
    "    for lemma in frequency_list:\n",
    "        rel = ''\n",
    "        matr = []\n",
    "        sk = int(lemma[1])\n",
    "        s = count_S(lemma[0], text) + sk\n",
    "        ratio = round((sk*100)/s,4)\n",
    "        if round((k*100)/t,4) > ratio:\n",
    "            rel = 'rep'\n",
    "        else:\n",
    "            rel = 'attr'\n",
    "        matr = [[sk, k-sk], [s-sk, t-s-k+sk]]\n",
    "        oddsr, fish = stats.fisher_exact(matr)\n",
    "        data.append([lemma[0], str(ratio), rel, str(format(fish,'.2e'))])\n",
    "    return data\n",
    "\n",
    "def write_results(data, output_file):\n",
    "    with open(output_file, 'w+', encoding='utf-8') as f:\n",
    "        for i in data:\n",
    "            f.write('\\t'.join(i) + '\\n')\n",
    "\n",
    "# input.txt -- файл с лексемами и их частотой\n",
    "write_results(count_fisher(read_frequency_list('input.txt'), k, t), 'output.txt')"
   ]
  },
  {
   "source": [
    "Тест Фишера для ковариации колексем\n",
    "Входные данные имеют формат:\n",
    "\n",
    "|лемма в слоте 1|частота|лемма в слоте 2|частота|лемма в слоте 1 и лемма в слоте 2|частота|"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, word: str, count: int):\n",
    "        self.word = word\n",
    "        self.count = count\n",
    "\n",
    "class Constr:\n",
    "    def __init__(self, w1: str, w2: str, count: int):\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        self.count = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frequency_list(filename) -> list:\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    for i in range(len(lines)):\n",
    "        lines[i] = lines[i].split('\\t')\n",
    "    del lines[0]\n",
    "    del lines[-1]\n",
    "    return lines\n",
    "\n",
    "def find(lemma: str, l: list) -> int or None:\n",
    "    for word in l:\n",
    "        if word.word == lemma:\n",
    "            return word.count\n",
    "    return None\n",
    "\n",
    "def count_fisher(lines) -> list:\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    l1l2 = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line[0] != '':\n",
    "            l1.append(Word(line[0], int(line[1])))\n",
    "        if line[2] != '':\n",
    "            l2.append(Word(line[2], int(line[3])))\n",
    "        if line[4] != '':\n",
    "            l1l2.append(Constr(line[4].split()[0], line[4].split()[1], int(line[5])))\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # t -- общее кол-во вхождений конструкции\n",
    "\n",
    "    for constr in l1l2:\n",
    "        sk = constr.count\n",
    "        k = find(constr.w2, l2)\n",
    "        s = find(constr.w1, l1)\n",
    "        matr = [[sk, k-sk], [s-sk, t-s-k+sk]]\n",
    "        rel = 'attr' if sk > ((s * k) / t) else 'rep'\n",
    "        oddsr, fish = stats.fisher_exact(matr)\n",
    "        data.append([constr.w1 + ' ' + constr.w2, rel, str(-math.log10(fish)), str(format(fish,'.2e'))])\n",
    "\n",
    "def write_results(data, output_file):\n",
    "    with open(output_file, 'w+', encoding='utf-8') as f:\n",
    "        for i in data:\n",
    "            f.write('\\t'.join(i) + '\\n')\n",
    "\n",
    "write_results(count_fisher(read_frequency_list('input.txt')), 'output.txt')"
   ]
  }
 ]
}